**一、预训练与微调的关系**

截止到现在，我们遇到过的名称：**Post-pretraining、Post-training、Pre-training、Fine-tuning、Continue Training。**这几个概念的关系可以归纳如下：

- **Pre-training（预训练）：**

- - **定义：** 大模型的基础训练阶段，使用海量通用语料进行自监督学习。
  - **目标：** 学习通用语言表示，构建一个通用语言模型（Base Model）。
  - **方法：** Masked Language Modeling (MLM)、Next Sentence Prediction (NSP)、多任务学习等。

- **Post-pretraining（后预训练，选用）：**

- - **定义：** 在 Pre-training 基础上，使用特定领域数据对模型进行额外训练。
  - **目标：** 提高模型在特定领域的表现能力，使其更适合专业任务或特定语境（如医疗、法律、金融等领域）。
  - **方法：** 领域数据的预训练、调整预训练任务。

- **Fine-tuning（微调）：**

- - **定义：** 通过小规模的任务特定数据集优化模型在具体任务上的性能。
  - **目标：** 使模型从“领域适配”进一步到“任务适配”。
  - **方法：** 全参数微调、冻结参数训练（如 LoRA、QLoRA 等）、增量学习。

- **Post-training（后训练，选用）：**

- - **定义：** 基于已训练模型，通过特定优化方法提升其能力，如指令理解、偏好优化等。
  - **目标：** 在某些特定能力上（如对话能力）进一步细化提升。
  - **方法：** 监督指令微调（SFT）、强化学习与人类反馈（RLHF）、拒绝采样（Rejection Sampling）。

- **Continue Training（继续训练，任意点可用）：**

- - **定义：** 对模型的进一步训练，可以在任意阶段应用，用于解决未完成的训练目标或适应新数据。
  - **目标：** 灵活调整模型以适应新需求，如补充训练数据、修复问题或适应环境变化。
  - **方法：** 数据扩充、任务调整等。

**对比总结**

- Pre-training 和 Fine-tuning 是核心的两步流程。

- Post-pretraining 和 Post-training 是针对特定场景的优化策略。

- Continue training 贯穿始终，用于动态更新和性能提升。

- 各阶段可以灵活组合，具体取决于任务需求和数据可用性。

- 以下是一个完整的训练流程示例：

  \1. Pre-training：在大规模开放域语料上训练基础模型。

  2.（可选）Post-pretraining：在医疗领域语料上适配，生成领域特化模型。

  3.Fine-tuning：用具体医疗任务（如诊断生成）的标注数据进行训练。

  4.（可选）Post-training：通过人类反馈或指令调优，让模型生成的诊断报告更加人性化。

  5.（可选）Continue training：补充新医学研究数据，保持模型知识更新。



**二、Post-training 与 Fine-tuning**

\1. **Post-training（后训练）**

- **工具：**Transformers 库、DeepSpeed Chat、TRL（Transformers Reinforcement Learning）、OpenAI Fine-tuning

- **方法：**

- - **监督指令微调（Supervised Fine-Tuning, SFT）：** 使用标注数据集优化模型指令理解能力。
  - **强化学习与人类反馈（Reinforcement Learning with Human Feedback, RLHF）：** 借助用户偏好优化模型输出。
  - **直接偏好优化（Direct Preference Optimization, DPO）：** 高效实现偏好优化。
  - **拒绝采样（Rejection Sampling）：** 通过剔除低质量生成数据提升结果可靠性。
  - **知识蒸馏（Knowledge Distillation）：** 压缩教师模型知识到后训练模型中，提升性能。

**2. Fine-tuning（微调）**

- **工具：**Hugging Face Transformers、AdapterHub、DeepSpeed、PyTorch Lightning、PEFT

- **方法：**

- - **全参数微调：** 优化所有模型参数，适用于小规模模型。
  - **LoRA（Low-Rank Adaptation）：** 只训练插入的低秩矩阵，降低计算成本。
  - **QLoRA（Quantized LoRA）：** 在模型量化后应用 LoRA 技术，适用于大规模模型高效微调。
  - **Prefix-Tuning：** 插入可学习的前缀，仅对前缀参数优化。
  - **Adapter Tuning：** 在模型的 Transformer 层之间加入轻量级适配器模块。
  - **P-Tuning v2：** 利用可学习的提示优化下游任务表现。
  - **增量学习（Incremental Learning）：** 对新任务数据进行补充训练，避免遗忘已有知识。

**对比总结**

- **Post-training：** 更偏向于提升模型能力，如指令理解和生成偏好优化；适用于后期特定需求的进一步优化。
- **Fine-tuning：** 更聚焦于特定任务的性能优化，适用范围广且方法多样，如全参数优化和高效参数微调。

**补充：知识蒸馏**

- **蒸馏与微调的关系：**

- 1. 蒸馏属于 Fine-tuning 还是 Post-training？

     视目标而定：若目的是压缩模型规模，可视为 Post-training。若目的是优化任务性能，可视为 Fine-tuning。

  2. **与微调(FT/SFT)的区别：**

- - **知识蒸馏：**更偏向模型压缩和推理加速。
  - **Fine-tuning：**更注重任务性能的提升。

- **蒸馏工具：**

- - **Transformers：**支持教师-学生模型蒸馏过程。
  - **Distiller：**开源框架，支持多种蒸馏策略。
  - **PyTorch Lightning：**可实现自定义蒸馏管道。

- **蒸馏与微调的先后顺序：**

- 1. **先蒸馏再微调：**适合需要高效推理的小型模型。
  2. **先微调再蒸馏：**适合任务对精度要求较高的场景。



**三、微调之间的关系**

**Fine-tuning（微调）：**

- **对象：**主要针对基础模型（Base Model）。
- **目标：**通过特定任务数据优化模型性能，例如分类、翻译或问答任务。
- **典型方法：**全参数微调、LoRA、QLoRA、Adapter Tuning 等。
- **适用场景：**单一任务优化，任务数据充足，模型需要适配特定领域。

**Instruct-tuning（指令微调）：**

- **对象：**专门针对指令模型（Instruct Model）。
- **目标：**提升模型的指令理解能力和对话生成表现，适用于多任务和对话场景。
- **典型方法：**监督训练、RLHF等。
- **适用场景：**增强模型对复杂指令的响应能力或多任务处理能力，特别是对话生成场景

**QLoRA和LoRA其实属于微调方法，而GPTQ、AQLM、AWQ往往用于做量化，**所以选择时需要根据目标来选择，如果追求**高精度**可以使用Lora与 QloRA，追求**更高的推理效率**建议GPTQ、AQLM、AWQ。



| **方法**  | **微调能力**                                    | **量化能力**             | **适用模型规模** | **目标精度**               | **硬件环境要求**                          | **引擎适配**                  |
| --------- | ----------------------------------------------- | ------------------------ | ---------------- | -------------------------- | ----------------------------------------- | ----------------------------- |
| **LoRA**  | 高效参数微调，更新部分低秩矩阵                  | 无量化能力               | 中小规模模型     | 高精度（接近全参数微调）   | GPU 内存要求较低，适合单 GPU 或小集群     | Transformers、AdapterHub      |
| **QLoRA** | 微调与量化结合，在 4-bit 量化基础上应用低秩适配 | 4-bit 量化               | 大规模模型       | 近高精度（略低于 LoRA）    | 需支持 4-bit 量化，资源占用低，适合大模型 | Transformers、DeepSpeed       |
| **GPTQ**  | 不支持微调，仅适合推理                          | 4-bit 或 8-bit 量化      | 任意规模模型     | 中等精度（更注重推理效率） | 对硬件友好，适合大规模推理任务            | Transformers、vLLM、sglang    |
| **AQLM**  | 不支持微调，仅适合推理                          | 高效量化，专注推理       | 大规模模型       | 较高精度（优于 GPTQ）      | 对硬件有一定要求，适合高效推理            | 专注实验性优化引擎，如 sglang |
| **AWQ**   | 不支持微调，仅适合推理                          | 自适应量化，适应动态负载 | 大规模模型       | 中等精度（与 GPTQ 类似）   | 硬件适配灵活，推理性能优秀                | Transformers、实验引擎        |



**各项技术可以叠加实现，且理论上会带来性能的提升，不同技术的叠加效果可能是互补的，例如微调与量化结合，或者微调与知识蒸馏结合，可以在保持高精度的同时提高推理效率和降低资源消耗，但叠加的效果会依赖于所选技术的特性以及应用场景。因此，需要具体问题具体分析，合理选择技术组合。**